---
title: "Project DS"
author: "Dimas Herlambang(123190140) & Ahmed Farel H(123190152)"
date: "11/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tm)
library(twitteR)
library(rtweet)
library(shiny) 
library(syuzhet)
library(wordcloud)
library(tm)
library(vroom)
library(here)
library(plyr)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(RTextTools)
```

```{r}
api_key<- "tkU2hmSgKfcrpG6HmOr7lDXqH"
api_secret<- "HU8kBfHmSuIrHvvZJ1ACyPPE72waXLLmWmdME8HFnVGVD3Np59"
access_token<- "1268143356271443968-XsqG2SPcAATdNE14jQDr1HDX7jJokr"
access_token_secret<- "aQDR7Mg3RmkqoK0cxAcOa9ON2MFwibGpHu9kvHpzAEYpD"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
```

```{r}
twit = searchTwitter('PrayForBatu', 
                   n = 180,
                   retryOnRateLimit = 10e5, lang = "id") #retryOnRateLimit untuk looping
saveRDS(twit,file = 'new_tweet.rds')
```

```{r}
twit <- readRDS('new_tweet.rds')
#convert twitteR list ke data
data = twListToDF(twit) 
#menampilkan semua tweet yang kita mining
komen <- data$text
komen1 <- Corpus(VectorSource(komen))

##hapus URL
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
twitclean <- tm_map(komen1, removeURL)
##hapus New Line
removeNL <- function(y) gsub("\n", "", y)
twitclean <- tm_map(twitclean, removeNL)
##hapus koma
replacecomma <- function(y) gsub(",", "", y)
twitclean <- tm_map(twitclean, replacecomma)
##hapus retweet
removeRT <- function(y) gsub("RT ", "", y)
twitclean <- tm_map(twitclean, removeRT)
##hapus titik
removetitik2 <- function(y) gsub(":", "", y)
twitclean <- tm_map(twitclean, removetitik2)
##hapus titik koma
removetitikkoma <- function(y) gsub(";", " ", y)
twitclean <- tm_map(twitclean, removetitikkoma)
#hapus titik3
removetitik3 <- function(y) gsub("p.", "", y)
twitclean <- tm_map(twitclean, removetitik3)
#hapus &amp
removeamp <- function(y) gsub("&amp;", "", y)
twitclean <- tm_map(twitclean, removeamp)
#hapus Mention
removeUN <- function(z) gsub("@\\w+", "", z)
twitclean <- tm_map(twitclean, removeUN)
#hapus space dll
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
twitclean <-tm_map(twitclean,stripWhitespace)
inspect(twitclean[1:10])
twitclean <- tm_map(twitclean,remove.all)
#hapus tanda baca
twitclean <- tm_map(twitclean, removePunctuation) 
#mengubah huruf kecil
twitclean <- tm_map(twitclean, tolower)
myStopwords <- readLines("stop.txt", warn = FALSE)
twitclean <- tm_map(twitclean,removeWords,myStopwords)

#HAPUS DATA KOSONG
try.error = function(x)
{
  # create missing value
  y = NA
  # tryCatch error
  try_error = tryCatch(tolower(x), error=function(e) e)
  # if not an error
  if (!inherits(try_error, "error"))
    y = tolower(x)
  # result
  return(y)
}
# lower case using try.error with sapply 
twitclean = sapply(twitclean, try.error)
# remove NAs in some_txt
twitclean = twitclean[!is.na(twitclean)]
names(twitclean) = NULL
```

```{r}
# dataframe data yg sudah bersih
dataframe<-data.frame(text=unlist(sapply(twitclean, `[`)), stringsAsFactors=F)
View(dataframe)
write.csv(dataframe,'tweet_bersih.csv')
```

```{r}
## Naive Bayes
library(e1071)
library(caret)
library(syuzhet)

#digunakan untuk membaca file csv yang sudah di cleaning data
tweet_dataset <-read.csv("tweet_bersih.csv",stringsAsFactors = FALSE)

#digunakan untuk mengeset variabel cloumn text menjadi char
review <- as.character(tweet_dataset$text)

#memanggil sentimen dictionary untuk menghitung presentasi dari beberapa emotion dan mengubahnya ke dalam text file
#anger, anticipation, disgust, fear, joy, sadness, surprise, trust, negative, positive
s<-get_nrc_sentiment(review)
review_combine<-cbind(tweet_dataset$text,s)#matrix
par(mar=rep(3,4))
barplot(colSums(s),col=rainbow(10),ylab='count',main='Analisis Sentimen')
```

```{r}
## Wordcloud

library(tm) #library untuk penggunaan corpus dalam cleaning data
library(RTextTools) #library untuk penggunaan corpus dalam cleaning data
library(e1071) #library yang terdapat sebuah algoritma naivebayes
library(dplyr) #library yang terdapat sebuah algoritma naivebayes
library(caret) #library yang terdapat sebuah algoritma naivebayes
library(shiny) #library untuk memanggil shinny
library(vroom) #membersihkan data
library(here)  #membersihkan data
library(ggplot2) #membuat barplot, grafik, dll
library(plotly)
library(syuzhet)

df<-read.csv("tweet_bersih.csv",stringsAsFactors = FALSE) #membaca file CSV
glimpse(df) #melihat tipe dan struktur objek.

#Atur seed generator bilangan acak R, yang berguna untuk membuat simulasi atau objek acak yang dapat direproduksi.
set.seed(20) #seed sebesar 20
df<-df[sample(nrow(df)),] 
df<-df[sample(nrow(df)),]
glimpse(df) #melihat tipe data dan struktur objek.
df$X=as.factor(df$X) #mengubah menjadi faktor
#menampilkan semua tweet yang kita mining
corpus<-Corpus(VectorSource(df$text)) 
corpus
#melihat data yang telah di corpus
inspect(corpus[1:10])

#fungsinya untuk membersihkan data data yang tidak dibutuhkan, menyaring data yang sebelumnya belum tersaring
corpus.clean<-corpus%>%
  tm_map(content_transformer(tolower))%>% #digunakan untuk mengubah huruf besar dari string menjadi string huruf kecil
  tm_map(removePunctuation)%>% #menghapus anda baca
  tm_map(removeNumbers)%>% #menghapus nomor
  tm_map(removeWords,stopwords(kind="en"))%>% #menghapus stopwords
  tm_map(stripWhitespace) 
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
#klasifikasi dari data prayforbatu dengan melakukan data training dan data test
df.train<-df[1:90,]
df.test<-df[91:180,]
dtm.train<-dtm[1:90,]
dtm.test<-dtm[91:180,]
corpus.clean.train<-corpus.clean[1:90]
corpus.clean.test<-corpus.clean[91:180]
#biasanya teks diubah menjadi Document-Term Matrix (DTM) melalui proses yang bernama tokenization. Tokenization berfungsi memecah 1 kalimat menjadi beberapa term. term dapat berupa 1 kata, pasangan 2 kata, dan seterusnya. 
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5) #frekuensi kemunculan kata tersebut pada dokumen
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)

convert_count <- function(x){
  y<-ifelse(x>0,1,0)
  y<-factor(y,levels=c(0,1),labels=c("no","yes"))
  y
}
#data train dan test naive bayes
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
classifier<-naiveBayes(trainNB,df.train$X,laplace = 1)
library(wordcloud)
#menampikan tampilan dari worldcloud
wordcloud(corpus.clean,min.freq = 4,max.words=60,random.order=F,colors=brewer.pal(8,"Dark2"))
```


```{r}
## Shiny
#membuka file csv
twitter <- read.csv(file="tweet_bersih.csv",header=TRUE)
#membuka text file pada data frame twitter
tweet <- twitter$text

# bagian yang mengatur tampilan web, baik input maupun outpun yang akan ditampilkan dalam web app.
ui <- fluidPage(
  titlePanel("Analisis Sentimen Bencana di Batu"), #halaman judul dari fluid page
  mainPanel( #tab pada fluidpage
    #plot output : untuk scatterplot
    tabsetPanel(type = "tabs",
                tabPanel("Data Twitter", DT::dataTableOutput('tbl')), #tab berupa data clening twitter
                tabPanel("Scatterplot", plotOutput("scatterplot")), #tab berupa scatterplot/grafik
                tabPanel("Wordcloud", plotOutput("Wordcloud")) #tab berupa worldcloud
    )
  )
)


# SERVER
# Disinialah tempat dimana data akan dianalisis dan diproses lalu hasilnya akan ditampilkan atau diplotkan pada bagian mainpanel() ui yang telah dijelaskan sebelumnya.
server <- function(input, output) {
  
  # Output Data
  output$tbl = DT::renderDataTable({ 
    DT::datatable(twitter, options = list(lengthChange = FALSE)) # data akan ditampilkan dalam beberapa halaman.
  })
  
  #Barplot
  output$scatterplot <- renderPlot({tweet_dataset<-read.csv("tweet_bersih.csv",stringsAsFactors = FALSE)
  review <-as.character(tweet_dataset$text)
  s<-get_nrc_sentiment(review)
  review_combine<-cbind(tweet_dataset$text,s)
  par(mar=rep(3,4))
  barplot(colSums(s),col=rainbow(10),ylab='count',main='sentiment analisis')
  }, height=400)
  
  #WordCloud
  output$Wordcloud <- renderPlot({
   set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
glimpse(df)
df$X=as.factor(df$X)
corpus<-Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])

#fungsinya untuk membersihkan data data yang tidak dibutuhkan 
corpus.clean<-corpus%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords,stopwords(kind="en"))%>%
  tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-df[1:90,]
df.test<-df[91:180,]
dtm.train<-dtm[1:90,]
dtm.test<-dtm[91:180,]
corpus.clean.train<-corpus.clean[1:90]
corpus.clean.test<-corpus.clean[91:180]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)

convert_count <- function(x){
  y<-ifelse(x>0,1,0)
  y<-factor(y,levels=c(0,1),labels=c("no","yes"))
  y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
classifier<-naiveBayes(trainNB,df.train$X,laplace = 1)
library(wordcloud)
wordcloud(corpus.clean,min.freq = 4,max.words=60,random.order=F,colors=brewer.pal(8,"Dark2"))
})

}

shinyApp(ui = ui, server = server)
```

